{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88cd4cb",
   "metadata": {},
   "source": [
    "# <p align=\"center\">Co-clustering on LBM generated data</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d7397",
   "metadata": {},
   "source": [
    "No files required to run this notebook.\n",
    "\n",
    "Required librairies : numpy, pandas, os (only for GPU usage), itertools, matplotlib, torch, kmeans_pytorch, sparsebm, sklearn\n",
    "\n",
    "In this notebook, you can generate data according to the Latent block model from sparsebm library and run our clustering model (Training Model). There is also the visualization and the comparaison with LBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a092a3",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8cf755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "## VISUALIZATION ##\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "## TORCH ##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "## KMEANS ##\n",
    "from kmeans_pytorch import kmeans\n",
    "\n",
    "## SPARSEBM ##\n",
    "from sparsebm import LBM\n",
    "from sparsebm.utils import CARI\n",
    "from sparsebm import generate_LBM_dataset\n",
    "\n",
    "## ARI ##\n",
    "from sklearn.metrics.cluster import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348501a6",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters for the GPU ##\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = torch.device('cuda:0')\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters of the data ##\n",
    "\n",
    "L = 3   # Numbers of clusters in rows\n",
    "Q = 4   # Numbers of clusters in columns\n",
    "M = 600 # Numbers of rows\n",
    "P = 900 # Numbers of columns\n",
    "D = 2   # Dimension of the latent space\n",
    "\n",
    "## Parameters of the encoder ##\n",
    "\n",
    "input_dim1 = M\n",
    "input_dim2 = P\n",
    "hidden1_dim1 = 48\n",
    "hidden1_dim2 = 48\n",
    "\n",
    "hidden2_dim1 = D\n",
    "hidden2_dim2 = 1\n",
    "\n",
    "## Parameters of the decoder ##\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "## Parameters of the training ##\n",
    "\n",
    "pre_epoch = 500\n",
    "learning_rate = 0.01\n",
    "num_epoch = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299f9dc",
   "metadata": {},
   "source": [
    "## Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a3baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent(X, Y):\n",
    "    \"\"\"\n",
    "    Plot the latent space of the rows and columns.\n",
    "    arguments :\n",
    "        X : rows latent positions (Tensor of shape (M, D))\n",
    "        Y : columns latent positions (Tensor of shape (P, D))\n",
    "    \"\"\"\n",
    "    plt.scatter(X[:, 0], X[:, 1], c='seagreen', marker='X', label='Rows')\n",
    "    plt.scatter(Y[:, 0], Y[:, 1], c='orange', marker='o', label='Colums')\n",
    "    plt.xlabel('X-axis')\n",
    "    plt.ylabel('Y-axis')\n",
    "    plt.title('Scatter plot of the latent space')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_block_matrix(A, r, c):\n",
    "    \"\"\"\n",
    "    Plot the adjacency matrix order by the clusters.\n",
    "    arguments :\n",
    "        A : Adjacency matrix (Tensor of shape (M, P))\n",
    "        r : cluster membership of the first group (Tensor of shape (M))\n",
    "        c : cluster membership of the second group (Tensor of shape (P))\n",
    "    \"\"\"\n",
    "    # Sort the adjency matrix by the cluster membership\n",
    "    A=A[np.argsort(r),:]\n",
    "    A=A[:,np.argsort(c)]\n",
    "\n",
    "    # Plot the matrix\n",
    "    plt.figure(figsize=(7, 8))\n",
    "    plt.title(\"Adjency matrix order by clusters\")\n",
    "    plt.imshow(A, cmap = \"cividis\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(loss):\n",
    "    \"\"\"\n",
    "    Plot the loss\n",
    "    arguments:\n",
    "        loss : Tensor containing the loss values for each epoch (Tensor of shape (epoch,))\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss.numpy(), label='Loss')\n",
    "    plt.title('Loss during pretraining')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "## PLOTTING FUNCTIONS ##\n",
    "\n",
    "def plot_training_loss(loss, loss1, loss2, loss3, loss4, loss5):\n",
    "    \"\"\"\n",
    "    Plot the training loss and its components.\n",
    "    arguments :\n",
    "        loss : Tensor containing the total loss values for each epoch (Tensor of shape (epoch,))\n",
    "        loss1 : Tensor containing the reconstruction loss values for each epoch (Tensor of shape (epoch,))\n",
    "        loss2 : Tensor containing the KL divergence loss values for rows for each epoch (Tensor of  shape (epoch,))\n",
    "        loss3 : Tensor containing the KL divergence loss values for columns for each epoch (Tensor of shape (epoch,))\n",
    "        loss4 : Tensor containing the cluster loss values for rows for each epoch (Tensor of shape (epoch,))\n",
    "        loss5 : Tensor containing the cluster loss values for columns for each epoch (Tensor of shape (epoch,))\n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(1, figsize=(15, 10))\n",
    "    ax.remove()\n",
    "    plt.subplot(251)\n",
    "    plt.plot(loss1.cpu().data.numpy(), color='royalblue')\n",
    "    plt.title(\"Reconstruction loss1\")\n",
    "    \n",
    "    plt.subplot(252)\n",
    "    plt.plot(loss2.cpu().data.numpy(), color='royalblue')\n",
    "    plt.title(\"KL loss2\")\n",
    "    \n",
    "    plt.subplot(253)\n",
    "    plt.plot(loss3.cpu().data.numpy(), color='royalblue')\n",
    "    plt.title(\"KL loss3\")\n",
    "    \n",
    "    plt.subplot(254)\n",
    "    plt.plot(loss4.cpu().data.numpy(), color='royalblue')\n",
    "    plt.title(\"Cluster loss4\")\n",
    "    \n",
    "    plt.subplot(255)\n",
    "    plt.plot(loss5.cpu().data.numpy(), color='royalblue')\n",
    "    plt.title(\"Cluster loss5\")\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plt.plot(loss.cpu().data.numpy(), color='royalblue')\n",
    "    plt.title(\"Training loss in total\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_ARI(ari_row, ari_col):\n",
    "    \"\"\"\n",
    "    Plot the ARI score for rows and columns during training.\n",
    "    arguments :\n",
    "        ari_row : List containing the ARI score for rows for each epoch (List of length (epoch))\n",
    "        ari_col : List containing the ARI score for columns for each epoch (List of length (epoch))\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(ari_row, color='royalblue')\n",
    "    plt.title(\"ARI for gamma\")\n",
    "    plt.subplot(122)\n",
    "    plt.plot(ari_col, color='royalblue')\n",
    "    plt.title(\"ARI for delta\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_latent_clusters(X, Y, r, c):\n",
    "    \"\"\"\n",
    "    Plot the latent space of the rows and columns with clusters.\n",
    "    arguments :\n",
    "        X : rows latent positions (Tensor of shape (M, D))\n",
    "        Y : columns latent positions (Tensor of shape (P, D))\n",
    "        r : Cluster membership for the rows (Numpy array of shape (M,))\n",
    "        c : Cluster membership for the columns (Numpy array of shape (P,))\n",
    "    \"\"\"\n",
    "\n",
    "    # Custom colormap\n",
    "    oranges = ['sienna', 'purple', 'goldenrod', 'deeppink', 'r']\n",
    "    cmap_orange = LinearSegmentedColormap.from_list(\"custom_cmap\", oranges)\n",
    "    greens = ['yellowgreen', 'mediumseagreen', 'darkgreen']\n",
    "    cmap_green = LinearSegmentedColormap.from_list(\"custom_cmap\", greens)\n",
    "\n",
    "    # Plot the latent space with clusters\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(X[:, 0].detach().numpy(), X[:, 1].detach().numpy(), c=r, cmap=cmap_green, marker='o', label='Rows')\n",
    "    plt.scatter(Y[:, 0].detach().numpy(), Y[:, 1].detach().numpy(), c=c, cmap=cmap_orange, marker='^', label='Columns')\n",
    "    plt.title(\"Latent representation with estimated clusters\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d2bf9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88882e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL ELEMENTS ##\n",
    "\n",
    "def glorot_init(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Glorot (or Xavier) initialization for weight parameters.\n",
    "    arguments:\n",
    "        input_dim : Dimension of the input (Integer)\n",
    "        output_dim : Dimension of the output (Integer)\n",
    "    return:\n",
    "        nn.Parameter : Initialized weight parameter (nn.Parameter object)\n",
    "    \"\"\"\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = torch.rand(input_dim, output_dim, dtype = torch.float32) * 2 * init_range - init_range\n",
    "    #initial = initial.to(device)\n",
    "    return nn.Parameter(initial)\n",
    "\n",
    "class GraphConvSparse(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Layer for the encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, activation = F.relu, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the layer.\n",
    "        arguments:\n",
    "            input_dim : Dimension of the input (Integer)\n",
    "            output_dim : Dimension of the output (Integer)\n",
    "            adj : Adjacency matrix of the graph (Tensor of shape (N, input_dim))\n",
    "            activation : Activation function to apply after convolution (Torch function, default is ReLU)\n",
    "        \"\"\"\n",
    "        super(GraphConvSparse, self).__init__(**kwargs)\n",
    "        self.weight = glorot_init(input_dim, output_dim)\n",
    "        self.adj = adj\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the layer.\n",
    "        arguments:\n",
    "            inputs : Input tensor of shape (N, input_dim)\n",
    "        return:\n",
    "            outputs : Result tensor of shape (N, output_dim)\n",
    "        \"\"\"\n",
    "        x = inputs\n",
    "        x = torch.mm(x,self.weight)\n",
    "        x = torch.mm(self.adj, x)\n",
    "        outputs = self.activation(x)\n",
    "        return outputs\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    GCN Encoder\n",
    "    This encoder contains two graph convolutional layers for encoding the input data.\n",
    "    It computes the mean and log standard deviation with two separate second layers and samples from a Gaussian distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, adj):\n",
    "        \"\"\"\n",
    "        Initialize the encoder.\n",
    "        arguments:\n",
    "            adj : Normalized adjacency matrix of the graph (Tensor of shape (input_dim1, input_dim2))\n",
    "        \"\"\"\n",
    "        super(Encoder,self).__init__()\n",
    "        self.base_gcn1 = GraphConvSparse(input_dim1, hidden1_dim1, adj.T)\n",
    "        self.gcn_mean1 = GraphConvSparse(hidden1_dim1, hidden2_dim1, adj, activation=lambda x:x)\n",
    "        self.gcn_logstddev1 = GraphConvSparse(hidden1_dim1, hidden2_dim2, adj, activation=lambda x:torch.where(x > -3, x, -3))\n",
    "\n",
    "        self.base_gcn2 = GraphConvSparse(input_dim2, hidden1_dim2, adj)\n",
    "        self.gcn_mean2 = GraphConvSparse(hidden1_dim2, hidden2_dim1, adj.T, activation=lambda x:x)\n",
    "        self.gcn_logstddev2 = GraphConvSparse(hidden1_dim2, hidden2_dim2, adj.T, activation=lambda x:torch.where(x > -3, x, -3))\n",
    "\n",
    "    def encode1(self, A):\n",
    "        \"\"\"\n",
    "        Encode the matrix using the rows.\n",
    "        arguments:\n",
    "            A : Adjacency matrix of the graph (Tensor of shape (input_dim1, input_dim2))\n",
    "        return:\n",
    "            sampled_X : Sampled latent representation for the rows (Tensor of shape (input_dim1, hidden2_dim1))\n",
    "            mean_X : Mean of the latent representation (Tensor of shape (input_dim1, hidden2_dim1))\n",
    "            logstd_X : Log standard deviation of the latent representation (Tensor of shape (input_dim1, hidden2_dim1))\n",
    "        \"\"\"\n",
    "        Id = torch.eye(input_dim1)\n",
    "        hidden1 = self.base_gcn1(Id)\n",
    "        mean1 = self.gcn_mean1(hidden1)\n",
    "        logstd1 = self.gcn_logstddev1(hidden1)\n",
    "\n",
    "        gaussian_noise1 = torch.randn(A.size(0), hidden2_dim1)\n",
    "        sampled_X = gaussian_noise1*torch.exp(logstd1) + mean1\n",
    "\n",
    "        return sampled_X, mean1, logstd1\n",
    "\n",
    "    def encode2(self, A):\n",
    "        \"\"\"\n",
    "        Encode the matrix using the columns.\n",
    "        arguments:\n",
    "            A : Adjacency matrix of the graph (Tensor of shape (input_dim1, input_dim2))\n",
    "        return:\n",
    "            sampled_Y : Sampled latent representation for the columns (Tensor of shape (input_dim2, hidden2_dim2))\n",
    "            mean_Y : Mean of the latent representation (Tensor of shape (input_dim2, hidden2_dim2))\n",
    "            logstd_Y : Log standard deviation of the latent representation (Tensor of shape (input_dim2, hidden2_dim2))\n",
    "        \"\"\"\n",
    "        Id = torch.eye(input_dim2)\n",
    "        hidden2 = self.base_gcn2(Id)\n",
    "        mean2 = self.gcn_mean2(hidden2)\n",
    "        logstd2 = self.gcn_logstddev2(hidden2)\n",
    "\n",
    "        gaussian_noise2 = torch.randn(A.size(1), hidden2_dim1)\n",
    "        sampled_Y = gaussian_noise2*torch.exp(logstd2) + mean2\n",
    "\n",
    "        return sampled_Y, mean2, logstd2\n",
    "\n",
    "    def forward(self,A):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "        arguments:\n",
    "            A : Adjacency matrix of the graph (Tensor of shape (input_dim1, input_dim2))\n",
    "        return:\n",
    "            X : Sampled latent representation for the rows (Tensor of shape (input_dim1, hidden2_dim1))\n",
    "            Y : Sampled latent representation for the columns (Tensor of shape (input_dim2, hidden2_dim2))\n",
    "            mean_X : Mean of the latent representation for the rows (Tensor of shape (input_dim1, hidden2_dim1))\n",
    "            logstd_X : Log standard deviation of the latent representation for the rows (Tensor of shape (input_dim1, hidden2_dim1))\n",
    "            mean_Y : Mean of the latent representation for the columns (Tensor of shape (input_dim2, hidden2_dim2))\n",
    "            logstd_Y : Log standard deviation of the latent representation for the columns (Tensor of shape (input_dim2, hidden2_dim2))\n",
    "        \"\"\"\n",
    "        X, mean_X, logstd_X = self.encode1(A)\n",
    "        Y, mean_Y, logstd_Y = self.encode2(A)\n",
    "\n",
    "        return X, Y, mean_X, logstd_X, mean_Y, logstd_Y\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder for the model.\n",
    "    This decoder computes the probability of edges between the latent representations of rows and columns using the distance (LPM).\n",
    "    It uses a parameter alpha to control the threshold for edge formation.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha):\n",
    "        \"\"\"\n",
    "        Initialize the decoder.\n",
    "        arguments:\n",
    "            alpha : Parameter to control the threshold for edge formation (Float)\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.tensor(alpha, dtype = torch.float32), requires_grad=True)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "        arguments:\n",
    "            X : Sampled latent representation for the rows (Tensor of shape (N1, N2))\n",
    "            Y : Sampled latent representation for the columns (Tensor of shape (N3, N2))\n",
    "        return:\n",
    "            probs : Probability of edges between the rows and columns (Tensor of shape (N1, N3))\n",
    "        \"\"\"\n",
    "        # Calcul of the euclidean distance squared between each pair\n",
    "        X_exp = X.unsqueeze(1)\n",
    "        Y_exp = Y.unsqueeze(0)\n",
    "        diff = X_exp - Y_exp\n",
    "        norm_sq = torch.sum(diff**2, dim=2)\n",
    "\n",
    "        # Calculation of the probability (sigmoid(alpha - ||X-Y||^2) )\n",
    "        term = self.alpha - 0.5*norm_sq\n",
    "        probs = torch.sigmoid(term)\n",
    "    \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28478d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VAE Model ##\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder (VAE) model for graph data.\n",
    "    It uses graph convolutional network for encoding and a distance-based decoder for reconstructing the adjacency matrix.\n",
    "    \"\"\"\n",
    "    def __init__(self, adj_norm, alpha):\n",
    "        \"\"\"\n",
    "        Initialize the VAE model.\n",
    "        arguments:\n",
    "            adj_norm : Normalized adjacency matrix of the graph (Tensor of size (M, P))\n",
    "            alpha : Parameter to control the threshold for edge formation in the decoder (Float)\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        self.adj_norm = adj_norm\n",
    "        self.encoder = Encoder(adj_norm)\n",
    "        self.decoder = Decoder(alpha)\n",
    "\n",
    "        # Initialization of the parameters of the clusters\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.FloatTensor(M,L).fill_(1.0/L), requires_grad=False)\n",
    "        self.delta = nn.Parameter(torch.FloatTensor(P,Q).fill_(1.0/Q), requires_grad=False)\n",
    "\n",
    "        self.pi = nn.Parameter(torch.FloatTensor(L).fill_(1.0/L), requires_grad=False)\n",
    "        self.tau = nn.Parameter(torch.FloatTensor(Q).fill_(1.0/Q), requires_grad=False)\n",
    "        \n",
    "        self.mu = nn.Parameter(torch.FloatTensor(np.random.multivariate_normal(np.zeros(D),np.eye(D),L)), requires_grad=False)\n",
    "        self.m = nn.Parameter(torch.FloatTensor(np.random.multivariate_normal(np.zeros(D),np.eye(D),Q)), requires_grad=False)\n",
    "\n",
    "        self.log_sigma = nn.Parameter(torch.FloatTensor(L).fill_(1), requires_grad=False)\n",
    "        self.log_s = nn.Parameter(torch.FloatTensor(Q).fill_(1), requires_grad=False)\n",
    "\n",
    "\n",
    "    def init_param(self, columns, rows, nbr_cluster_col, nbr_cluster_row) :\n",
    "\n",
    "        row_clusters, mu_km = kmeans(X=rows, num_clusters=nbr_cluster_row, distance='euclidean')\n",
    "        col_clusters, m_km = kmeans(X=columns, num_clusters=nbr_cluster_col, distance='euclidean')\n",
    "\n",
    "        gamma_km = F.one_hot(row_clusters)\n",
    "        delta_km = F.one_hot(col_clusters)\n",
    "\n",
    "        pi_km = torch.mean(gamma_km, dim=0, dtype= torch.float)\n",
    "        tau_km = torch.mean(delta_km, dim=0, dtype= torch.float)\n",
    "\n",
    "        sigma_km = torch.zeros(nbr_cluster_row)\n",
    "        for c in range(nbr_cluster_row) :\n",
    "            sigma_km[c] = torch.std(rows[row_clusters==c])\n",
    "\n",
    "        s_km = torch.zeros(nbr_cluster_col)\n",
    "        for c in range(nbr_cluster_col) :\n",
    "            s_km[c] = torch.std(columns[col_clusters==c])\n",
    "\n",
    "        log_sigma_km = torch.log(sigma_km)\n",
    "        log_s_km = torch.log(s_km)\n",
    "\n",
    "        return gamma_km, delta_km, pi_km, tau_km, mu_km, m_km, log_sigma_km, log_s_km\n",
    "    \n",
    "\n",
    "\n",
    "    def pretrain(self, A, adj_label, kl_weight = 0.0001):\n",
    "        \"\"\"\n",
    "        Pretrain the VAE model using the provided adjacency matrix and labels.\n",
    "        arguments:\n",
    "            A : Adjacency matrix of the graph (Tensor of size (M, P))\n",
    "            adj_label : Original adjacency matrix (used for computing loss) (Tensor of size (M, P))\n",
    "            kl_weight : Weight for the KL divergence term in the loss (Float) (default is 0.0001)\n",
    "        return:\n",
    "            X : Sampled latent representation for the rows (Tensor of size (M, D))\n",
    "            Y : Sampled latent representation for the columns (Tensor of size (P, D))\n",
    "            A_probs : Probability of edges between the rows and columns (Tensor of size (M, P))\n",
    "            store_pre_loss : Tensor to store the loss values for each epoch (Tensor of size (pre_epoch))\n",
    "            store_recon_loss : Tensor to store the reconstruction loss values for each epoch (Tensor of size (pre_epoch))\n",
    "            store_kl_loss : Tensor to store the KL divergence loss values for each epoch (Tensor of size (pre_epoch))\n",
    "        \"\"\"\n",
    "\n",
    "        # Adam optimizer\n",
    "        optimizer = Adam(itertools.chain(self.encoder.parameters(), self.decoder.parameters()), lr=learning_rate)\n",
    "\n",
    "        store_pre_loss = torch.zeros(pre_epoch)\n",
    "        store_kl_loss = torch.zeros(pre_epoch)\n",
    "        store_recon_loss = torch.zeros(pre_epoch)\n",
    "\n",
    "        # Weights for the positive and negative samples in the binary cross-entropy loss\n",
    "        pos_weight = float(adj_label.numel() - adj_label.sum()) / adj_label.sum()\n",
    "\n",
    "        # Pretraining loop\n",
    "        for epoch in range(pre_epoch):\n",
    "\n",
    "            # Encoder forward pass\n",
    "            X, Y, mean_X, logstd_X, mean_Y, logstd_Y = self.encoder(A)\n",
    "\n",
    "            # Decoder forward pass\n",
    "            A_probs = self.decoder(X, Y)\n",
    "\n",
    "            # Reconstruction loss\n",
    "            recon_loss = F.binary_cross_entropy(A_probs.view(-1),adj_label.view(-1), weight=pos_weight)\n",
    "\n",
    "            # KL divergence loss\n",
    "            kl_divergence_X = 0.5 * torch.mean( 2 * (- 1 - 2 * logstd_X + torch.exp(2 * logstd_X)) + mean_X.pow(2))\n",
    "            kl_divergence_Y = 0.5 * torch.mean( 2 * (- 1 - 2 * logstd_Y + torch.exp(2 * logstd_Y)) + mean_Y.pow(2))\n",
    "\n",
    "            # Total loss\n",
    "            loss = recon_loss - kl_weight * (kl_divergence_X + kl_divergence_Y)\n",
    "\n",
    "            # Model parameters update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            store_pre_loss[epoch] = loss.item()\n",
    "            store_kl_loss[epoch] = kl_weight * (kl_divergence_X + kl_divergence_Y)\n",
    "            store_recon_loss[epoch] = recon_loss\n",
    "            \n",
    "        # Initialize the parameters of the clusters\n",
    "        self.gamma.data, self.delta.data, self.pi.data, self.tau.data, self.mu.data, self.m.data, self.log_sigma.data, self.log_s.data = self.init_param(Y, X, Q, L)\n",
    "\n",
    "        print('Pretraining termin√© !')\n",
    "        return X, Y, A_probs, store_pre_loss, store_recon_loss, store_kl_loss\n",
    "    \n",
    "\n",
    "\n",
    "    def kullback_leibler_divergence(self, mean, mean_gcn, logstd, logstd_gcn):\n",
    "        \"\"\"\n",
    "        Compute the Kullback-Leibler divergence between two Gaussian distributions.\n",
    "        arguments :\n",
    "            mean : Mean of the clusters (Tensor of shape (N2, D))\n",
    "            mean_gcn : Mean of the GCN (Tensor of shape (N1, D))\n",
    "            logstd : Log standard deviation of the clusters (Tensor of shape (N2, 1))\n",
    "            logstd_gcn : Log standard deviation of the GCN (Tensor of shape (N1, 1))\n",
    "        return :\n",
    "            Kullback-Leibler divergence (Tensor of shape (N1, N2, D))\n",
    "        \"\"\"\n",
    "        \n",
    "        latent_dim = mean_gcn.shape[1]\n",
    "\n",
    "        var_gcn = torch.exp(2*logstd_gcn)\n",
    "        var = torch.exp(2*logstd) + 1e-16\n",
    "        \n",
    "        mean_gcn = mean_gcn.unsqueeze(1)\n",
    "        mean = mean.unsqueeze(0)\n",
    "\n",
    "        logvar_gcn = 2*logstd_gcn\n",
    "        logvar = 2*logstd\n",
    "\n",
    "        D_KL = 0.5 * (latent_dim * (logvar - logvar_gcn - 1 + var_gcn/var) + torch.sum((mean_gcn - mean)**2, dim=2) / var)\n",
    "\n",
    "        return D_KL\n",
    "\n",
    "\n",
    "\n",
    "    def update_var_probability(self, prior, mean, mean_gcn, log_std, logstd_gcn):\n",
    "        \"\"\"\n",
    "        Update the variationnal probability of individuals in the clusters.\n",
    "        arguments :\n",
    "            prior : Prior distribution\n",
    "            mean : Mean of the clusters\n",
    "            mean_gcn : Mean of the GCN\n",
    "            std : Log of standard deviation of the clusters\n",
    "            std_gcn : Log of standard deviation of the GCN\n",
    "        return :\n",
    "            Updated variational probability for each individual in each cluster (Tensor of shape (N2, N1, D))\n",
    "        \"\"\"\n",
    "        point_dim = mean_gcn.shape[0]\n",
    "        cluster_dim = prior.shape[0]\n",
    "        latent_dim = mean_gcn.shape[1]\n",
    "        \n",
    "        var_prob = torch.zeros((point_dim, cluster_dim, latent_dim), dtype=torch.float32)\n",
    "        dkl = self.kullback_leibler_divergence(mean, mean_gcn, log_std, logstd_gcn)\n",
    "\n",
    "        log_numerator = torch.log(prior) - dkl\n",
    "        max_log_numerator = torch.max(log_numerator, dim=1, keepdim=True).values\n",
    "        log_denominator = max_log_numerator + (torch.logsumexp(torch.log(prior) - dkl - max_log_numerator, dim=1, keepdim=True))\n",
    "        var_prob = torch.exp(log_numerator - log_denominator)\n",
    "    \n",
    "        return var_prob\n",
    "    \n",
    "\n",
    "    \n",
    "    def update_others_parameters(self, prob_var, mean, mean_gcn, logstd_gcn):\n",
    "        \"\"\"\n",
    "        Update the parameters of the clusters.\n",
    "        arguments :\n",
    "            prob_var : Variational probability for each individual in each cluster (Tensor of shape (N1, N2))\n",
    "            mean : Mean of the clusters (Tensor of shape (N2, D))\n",
    "            mean_gcn : Mean of the GCN (Tensor of shape (N2, D))\n",
    "            logstd_gcn : Log standard deviation of the GCN (Tensor of shape (N2, 1))\n",
    "        return :\n",
    "            prior : Updated prior distribution (Tensor of shape (N2,))\n",
    "            new_mean : Updated means of the clusters (Tensor of shape (N2, D))\n",
    "            new_logstd : Updated log of standard deviations of the clusters (Tensor of shape (N2, 1))\n",
    "        \"\"\"\n",
    "\n",
    "        latent_dim = mean_gcn.shape[1]\n",
    "    \n",
    "        prob_var_sum = torch.sum(prob_var, dim=0) + 1e-16\n",
    "        prior = prob_var_sum / (prob_var.shape[0] + 1e-16)\n",
    "\n",
    "        mean_gcn = mean_gcn.unsqueeze(1)\n",
    "        mean = mean.unsqueeze(0)\n",
    "\n",
    "        # Update the mean\n",
    "        new_mean = torch.sum(prob_var.unsqueeze(2) * mean_gcn, dim=0) / prob_var_sum.unsqueeze(1)\n",
    "    \n",
    "        # Update the standard deviation\n",
    "        numerator_mean = torch.sum(prob_var * (torch.sum((mean_gcn - mean)**2, dim=2)), dim=0)\n",
    "        mean_part = numerator_mean / (latent_dim * prob_var_sum + 1e-16)\n",
    "        # Calculation with logsumexp for numerical stability\n",
    "        max_numerator_std = torch.max(torch.log(prob_var) + 2*logstd_gcn, dim=0, keepdim=True).values\n",
    "        numerator_std = max_numerator_std + torch.logsumexp(torch.log(prob_var) + 2*logstd_gcn - max_numerator_std, dim=0)\n",
    "        denominator_std = torch.logsumexp(torch.log(prob_var), dim=0)\n",
    "        ratio_std = numerator_std - denominator_std\n",
    "        std_part = torch.exp(ratio_std)\n",
    "        \n",
    "        new_logstd = 0.5 * torch.log((mean_part + std_part))\n",
    "\n",
    "        return prior, new_mean, new_logstd\n",
    "    \n",
    "\n",
    "    \n",
    "    def train(self, A, A_label, row_cluster, column_cluster):\n",
    "        \"\"\"\n",
    "        Train the VAE model\n",
    "        arguments:\n",
    "            A : Adjacency matrix of the graph (Tensor of size (M, P))\n",
    "            adj_label : Original adjacency matrix (used for computing loss) (Tensor of size (M, P))\n",
    "            row_cluster : True cluster labels for the rows (Numpy array of size (M,))\n",
    "            column_cluster : True cluster labels for the columns (Numpy array of size (P,))\n",
    "        return:\n",
    "            X : Sampled latent representation for the rows (Tensor of size (M, D))\n",
    "            Y : Sampled latent representation for the columns (Tensor of size (P, D))\n",
    "            A_probs : Probability of edges between the rows and columns (Tensor of size (M, P))\n",
    "            store_loss : Tensor to store the loss values for each epoch (Tensor of size (num_epoch))\n",
    "            store_loss1 : Tensor to store the reconstruction loss values for each epoch (Tensor of size (num_epoch))\n",
    "            store_loss2 : Tensor to store the KL divergence loss values for rows for each epoch (Tensor of size (num_epoch))\n",
    "            store_loss3 : Tensor to store the KL divergence loss values for columns for each epoch (Tensor of size (num_epoch))\n",
    "            store_loss4 : Tensor to store the cluster loss values for rows for each epoch (Tensor of size (num_epoch))\n",
    "            store_loss5 : Tensor to store the cluster loss values for columns for each epoch (Tensor of size (num_epoch))\n",
    "            store_ari_gamma : List to store the ARI score for rows for each epoch (List of length (num_epoch))\n",
    "            store_ari_delta : List to store the ARI score for columns for each epoch (List of length (num_epoch)) \n",
    "        \"\"\"\n",
    "\n",
    "        # Adam optimizer\n",
    "        optimizer = Adam(itertools.chain(self.encoder.parameters(), self.decoder.parameters()), lr=learning_rate)\n",
    "\n",
    "        # Store the loss and Ari score\n",
    "        store_loss = torch.zeros(num_epoch)\n",
    "        store_loss1 = torch.zeros(num_epoch)\n",
    "        store_loss2 = torch.zeros(num_epoch)\n",
    "        store_loss3 = torch.zeros(num_epoch)\n",
    "        store_loss4 = torch.zeros(num_epoch)\n",
    "        store_loss5 = torch.zeros(num_epoch)\n",
    "        store_ari_gamma = []\n",
    "        store_ari_delta = []\n",
    "\n",
    "        # Training loop\n",
    "        for i in range(num_epoch):\n",
    "        \n",
    "            # Encoder forward pass\n",
    "            X, Y, mean_X, logstd_X, mean_Y, logstd_Y = self.encoder(A)\n",
    "\n",
    "            # Decoder forward pass\n",
    "            A_probs = self.decoder(X, Y)\n",
    "\n",
    "            # Update gamma and delta\n",
    "            self.gamma.data = self.update_var_probability(self.pi, self.mu, mean_X, self.log_sigma, logstd_X)\n",
    "            self.delta.data = self.update_var_probability(self.tau, self.m, mean_Y, self.log_s, logstd_Y)\n",
    "\n",
    "            if torch.isnan(self.gamma).any() or torch.isnan(self.delta).any():\n",
    "                print(\"NaN detected in gamma or delta\")\n",
    "                break\n",
    "\n",
    "            # Update pi, tau, mu, m, sigma and s\n",
    "            self.pi.data, self.mu.data, self.log_sigma.data = self.update_others_parameters(self.gamma, self.mu, mean_X, logstd_X)\n",
    "            self.tau.data, self.m.data, self.log_s.data = self.update_others_parameters(self.delta, self.m, mean_Y, logstd_Y)\n",
    "\n",
    "            # Loss of reconstruction\n",
    "            recon_loss = -torch.sum(A_label * (torch.log(A_probs + 1e-16)) + (1 - A_label) * (torch.log(1 - A_probs + 1e-16)))\n",
    "\n",
    "            # KL divergence loss\n",
    "            kl_divergence_X = torch.sum(self.gamma * self.kullback_leibler_divergence(self.mu, mean_X, self.log_sigma, logstd_X))\n",
    "            kl_divergence_Y = torch.sum(self.delta * self.kullback_leibler_divergence(self.m, mean_Y, self.log_s, logstd_Y))\n",
    "\n",
    "            # Cluster loss\n",
    "            loss4 = torch.sum(self.gamma * (torch.log(self.pi + 1e-16) - torch.log(self.gamma + 1e-16)))\n",
    "            loss5 = torch.sum(self.delta * (torch.log(self.tau + 1e-16) - torch.log(self.delta + 1e-16)))\n",
    "\n",
    "            # Total loss\n",
    "            loss = recon_loss -  kl_divergence_X - kl_divergence_Y + (loss4 + loss5)\n",
    "\n",
    "            # Model parameters update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store the loss value\n",
    "            store_loss[i] = torch.Tensor.item(loss)  # save train loss for visu\n",
    "            store_loss1[i] = torch.Tensor.item(recon_loss)\n",
    "            store_loss2[i] = torch.Tensor.item(torch.mean(kl_divergence_X))\n",
    "            store_loss3[i] = torch.Tensor.item(torch.mean(kl_divergence_Y))\n",
    "            store_loss4[i] = torch.Tensor.item(loss4)\n",
    "            store_loss5[i] = torch.Tensor.item(loss5)\n",
    "            \n",
    "            # Store the ARI score\n",
    "            store_ari_gamma.append(adjusted_rand_score(row_cluster, torch.argmax(self.gamma, axis=1).cpu().numpy()))\n",
    "            store_ari_delta.append(adjusted_rand_score(column_cluster, torch.argmax(self.delta, axis=1).cpu().numpy()))\n",
    "            \n",
    "            if (i+1) % 100 == 0 :\n",
    "                print(f\"Epoch: {i + 1:04d} / {num_epoch:.0f}  |  Loss: {loss.item():.5f}  |  Recon loss: {recon_loss.item():.5f} |  ARI delta: {store_ari_delta[-1]:.5f} | ARI gamma: {store_ari_gamma[-1]:.5f}\")\n",
    "                \n",
    "        return X, Y, A_probs, store_loss, store_loss1,store_loss2,store_loss3,store_loss4,store_loss5, store_ari_gamma, store_ari_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afffd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_adj(adj):\n",
    "  \"\"\"\n",
    "  Normalize the adjacency matrix.\n",
    "  arguments:\n",
    "      adj : Adjacency matrix of the graph (Tensor of shape (M, P))\n",
    "  return:\n",
    "      adj_normalized : Normalized adjacency matrix (Tensor of shape (M, P))\n",
    "  \"\"\"\n",
    "  \n",
    "  # Create an diagonal matrix with the sum of the rows of adj\n",
    "  rowsum = torch.sum(adj, dim=1)\n",
    "\n",
    "  # Create a diagonal matrix with the inverse of the sum of the rows of adj\n",
    "  row_degree_mat_inv_sqrt = torch.diag(1 / torch.sqrt(rowsum))\n",
    "\n",
    "  # Create an diagonal matrix with the sum of the columns of adj\n",
    "  colsum = torch.sum(adj, dim=0)\n",
    "\n",
    "  # Create a diagonal matrix with the inverse of the sum of the columns of adj\n",
    "  col_degree_mat_inv_sqrt = torch.diag(1 / torch.sqrt(colsum))\n",
    "\n",
    "  #Replace infinite value by a non-zero value\n",
    "  row_degree_mat_inv_sqrt[torch.isinf(row_degree_mat_inv_sqrt)] = 1e-6\n",
    "  col_degree_mat_inv_sqrt[torch.isinf(col_degree_mat_inv_sqrt)] = 1e-6\n",
    "\n",
    "  # Create the normalized adjacency matrix\n",
    "  adj_normalized = torch.mm(torch.mm(row_degree_mat_inv_sqrt, adj), col_degree_mat_inv_sqrt)\n",
    "\n",
    "  return adj_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a503cae",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data generation following LBM model ##\n",
    "\n",
    "# Probalities of connexion in clusters\n",
    "\n",
    "probabilities = (np.array([\n",
    "                            [0.15, 0.01, 0.02, 0.02],\n",
    "                            [0.05, 0.13, 0.03, 0.01],\n",
    "                            [0.1, 0.03, 0.15, 0.01],\n",
    "                        ]))\n",
    "\n",
    "# Generate the dataset\n",
    "\n",
    "dataset = generate_LBM_dataset(M,P, nb_row_clusters=L,nb_column_clusters=Q,connection_probabilities=probabilities, sparse = False)\n",
    "\n",
    "# Transform the sparse matrix to tensor\n",
    "\n",
    "sparse_matrix = dataset.data\n",
    "row = torch.tensor(sparse_matrix.row, dtype=torch.long)\n",
    "col = torch.tensor(sparse_matrix.col, dtype=torch.long)\n",
    "values = torch.tensor(sparse_matrix.data, dtype=torch.float32)\n",
    "indices = torch.stack([row, col])\n",
    "shape = sparse_matrix.shape\n",
    "sparse_tensor = torch.sparse_coo_tensor(indices, values, size=shape)\n",
    "A_lbm = sparse_tensor.to_dense()\n",
    "\n",
    "# Plot the adjacency matrix\n",
    "\n",
    "row_cluster_lbm = np.argmax(dataset.row_cluster_indicator, axis=1)\n",
    "col_cluster_lbm =np.argmax(dataset.column_cluster_indicator, axis=1)\n",
    "plot_block_matrix(sparse_tensor.to_dense().numpy(),row_cluster_lbm,col_cluster_lbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d670d3",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "\n",
    "adj_normalized = normalized_adj(A_lbm)\n",
    "model = VAE(adj_normalized, alpha)\n",
    "X, Y, A_pred, loss, rec_l, kl_l = model.pretrain(adj_normalized, A_lbm, 0.001)\n",
    "\n",
    "plot_latent(X.detach().numpy(), Y.detach().numpy())\n",
    "print(adjusted_rand_score(row_cluster_lbm, torch.argmax(model.gamma, axis=1).cpu().numpy()))\n",
    "print(adjusted_rand_score(col_cluster_lbm, torch.argmax(model.delta, axis=1).cpu().numpy()))\n",
    "\n",
    "# Train the model\n",
    "\n",
    "X, Y, A_probs, store_loss, store_loss1,store_loss2,store_loss3,store_loss4,store_loss5, store_ari_gamma, store_ari_delta = model.train(adj_normalized, A_lbm, row_cluster_lbm, col_cluster_lbm)\n",
    "\n",
    "# Results\n",
    "\n",
    "print(\"ARI for gamma:\", round(store_ari_gamma[-1],3))\n",
    "print(\"ARI for delta:\", round(store_ari_delta[-1],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency matrix with original clusters\n",
    "A_fin = torch.bernoulli(A_probs)\n",
    "plot_block_matrix(A_fin.detach().numpy(),row_cluster_lbm,col_cluster_lbm)\n",
    "\n",
    "# Adjacency matrix with estimated clusters\n",
    "c_est = torch.argmax(model.delta, axis=1).cpu().numpy()\n",
    "r_est = torch.argmax(model.gamma, axis=1).cpu().numpy()\n",
    "plot_block_matrix(A_fin.detach().numpy(), r_est, c_est)\n",
    "\n",
    "# Plot the latent space with clusters\n",
    "plot_latent_clusters(X, Y, r_est, c_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbffc662",
   "metadata": {},
   "source": [
    "## Comparaison with LBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c755fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparaison with LBM model results ##\n",
    "\n",
    "model_row_labels = torch.argmax(model.gamma, axis=1).cpu().numpy()\n",
    "model_column_labels = torch.argmax(model.delta, axis=1).cpu().numpy()\n",
    "\n",
    "lbm = LBM(L,Q,n_init_total_run=1, verbosity=False)\n",
    "lbm.fit(A_lbm)\n",
    "\n",
    "lbm_cari = CARI(row_cluster_lbm,col_cluster_lbm,lbm.row_labels,lbm.column_labels)\n",
    "model_cari = CARI(row_cluster_lbm,col_cluster_lbm,model_row_labels,model_column_labels)\n",
    "\n",
    "print(\"Our Model result :\")\n",
    "print(\"Adjusted Rand index for rows is {:.2f}\".format(store_ari_gamma[-1]))\n",
    "print(\"Adjusted Rand index for columns is {:.2f}\".format(store_ari_delta[-1]))\n",
    "print(\"Co-Adjusted Rand index is {:.2f}\".format(model_cari))\n",
    "\n",
    "print(\"\\nLBM result :\")\n",
    "print(\"Adjusted Rand index for rows is {:.2f}\".format(adjusted_rand_score(row_cluster_lbm, lbm.row_labels)))\n",
    "print(\"Adjusted Rand index for columns is {:.2f}\".format(adjusted_rand_score(col_cluster_lbm, lbm.column_labels)))\n",
    "print(\"Co-Adjusted Rand index from lbm is {:.2f}\".format(lbm_cari))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
